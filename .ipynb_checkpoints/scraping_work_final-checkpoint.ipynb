{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 0.3, started July 18 in 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://cispa.de/en/news-and-events/news-archive\")\n",
    "page = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting to how much news-sites are there \n",
    "number_retrieved = page.select(\".pagination\")[1].get('data-pagesize')\n",
    "number = int(number_retrieved)\n",
    "# testing \n",
    "# number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the list to automate URL construction\n",
    "number_of_pages = list(range(1,number+1))\n",
    "\n",
    "# testing: does it contain all numbers I need? From 1 to number+1 because of range()\n",
    "# number_of_pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list urls that specifiy the webpages, the archive consists of\n",
    "# I use a list comprehensionðŸ™ˆ \n",
    "\n",
    "newspages_list = [f'https://cispa.de/en/news-and-events/news-archive?newsPage={page_var}#_news' for page_var in number_of_pages]\n",
    "\n",
    "# testing: does it contain all URls? \n",
    "# newspages_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scraping every website of the archive with the following code\n",
    "\n",
    "# initializing the list for the results\n",
    "scraped_news = []\n",
    "\n",
    "for newspage in newspages_list:\n",
    "    # initializing list to test \n",
    "    test_list = []\n",
    "    # print(newspage)\n",
    "    # requesting one of the needed URLs\n",
    "    response = requests.get(newspage)\n",
    "    # each url leads to a website of the news archive I want to scrape\n",
    "    page = BeautifulSoup(response.text, 'html.parser')\n",
    "    # on this website I am interested in the class .news as it contains information\n",
    "    # about title, date and URL of each news\n",
    "    articles  = page.select(\".news\")\n",
    "    \n",
    "    # debugging: test line \n",
    "    #print(f'++++++ list has {len(articles)} elements ++++++')\n",
    "    \n",
    "    # for every news on a webpage I scrape date, title, URL to full test of news  \n",
    "    for article in articles: \n",
    "        # initializing the dictionary using to store scraped information\n",
    "        a_news = {}\n",
    "        date = article.select(\".h2\")[0].text.strip()\n",
    "        title = article.get('title')\n",
    "        url = article.get('href')\n",
    "        # filling the structure of the dictionary a_news with values  \n",
    "        a_news['date'] = date\n",
    "        a_news['title'] = title\n",
    "        a_news['url'] = url\n",
    "        # playing around with the list\n",
    "        test_list = [date, title, url]\n",
    "        \n",
    "        # putting a_news dictionary entry in the list of scraped_news\n",
    "        scraped_news.append(a_news)\n",
    "\n",
    "#         \n",
    "# testing the filling         \n",
    "#for item in scraped_news: \n",
    " #   print('START')\n",
    "  #  print(item)\n",
    "   # print('END')\n",
    "\n",
    "# testing the coverage: did I get all news? \n",
    "# print(len(scraped_news))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# putting the scraped stuff into a dataframe\n",
    "df = pd.DataFrame(scraped_news)\n",
    "# hoch does the dataframe df look? \n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the dataframe df to a comma-separated values (csv) file\n",
    "df.to_csv(\"scraped_news.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
